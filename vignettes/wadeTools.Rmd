---
title: "Getting Started with wadeTools"
author: "Wade Rogers"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  knitr.kable.NA = '',
  comment = ""
)
```

```{r eval=TRUE, echo=FALSE, message=FALSE}
# load some libraries early to suppress messages for the vignette
library(flowCore)
library(flowViz)
library(flowFP)
library(splancs)
library(sp)
library(fields)
library(KernSmooth)
library(wadeTools)
```

## Purpose of this package
The ***wadeTools*** package is a collection of a wide variety of R functions that I've written over the years.  They accomplish big or small tasks that I find that I do over and over again, so as in any software development work, it makes sense to write it once (carefully and with an eye towards re-useability) and use it often.  

I had originally created a 'tools' directory which had task-specific subdirectories (e.g. visualization, compensation, transformation, etc.).  Any time I found I had a function that seemed to have broader utility than just the project at hand, I dumped it into the tools hierarchy.  Over time functions and categories accumulated.  Mostly these had to do specifically with flow cytometry data processing, but a few didn't.

I've only recently turned this collection into a formal R package.  Why?  Because I wanted to share these tools with **Cytomics Workshop** participants - people who are interested in acquiring or honing  skills in developing and applying advanced computational analysis methods for flow cytometry.  In earlier iterations of the workshop I simply shared the tools directory.  That's akin to dumping a box of random tools on the workbench and asking you to build a barn, with no *a priori* knowledge of what a plane, or a saw, or a chisel was used for, or how to use them.  A package has several monumental advantages over this *ad hoc* approach to sharing code:

1.  it's easy to distribute via Github,
1.  it installs and loads just like any other R package,
1.  each function is documented with a help page that shows its arguments and usage, and in many cases examples of its use,
1.  it comes with this vignette - essentially a cook's tour of the package and how to get started with it.

## Roadmap of functions
First, here's a table of functions exported by the package (there are many more "helper" functions that aren't intended to be exposed to the user but provide capabilities for the public exported functions).  I've organized these public functions into categories (e.g. reading, processing, compensation, etc.), and highlighted the ones I find I use the most often in **bold**.  Consider this table to be a sort of reference "roadmap" to what's available.

```{r, echo=FALSE, results='asis'}
tfunc = read.csv("table_of_functions.csv")
knitr::kable(tfunc)
```

## A simple example
Let's illustrate the use of the package with a simple example.  Let's take a look at a single FCS file, which may be one of many files in a project.  But for now, just consider this one file (we'll look at collections of files in a little bit).  We will:

* read the file
* compensate the data using the SPILL matrix in the FCS header
* transform the data biexponentially
* take a look at a couple of pictures

We'll do this 2 ways.  First, using standard **flowCore** functions, and second, using **wadeTools**.  We'll retrieve the filename of the example data first, which we'll use for both methods:
```{r eval=TRUE, echo=TRUE}
filename = system.file("extdata", "example1.fcs", package = "wadeTools")
```

##### flowCore method
```{r, eval = TRUE, echo = TRUE, fig.show='hold'}
ff = read.FCS(filename)

# get the spillover matrix from the header
spill = keyword(ff)$SPILL
ff = compensate(ff, spillover = spill)

# apply the flowCore default biexponential transformation
bt = biexponentialTransform()
fft = transform(ff, transformList(colnames(ff)[7:22], bt))

# make a couple of pictures
plot(fft, c("FSC-A", "SSC-A"), main = "Scattering")
plot(fft, c("Green D 610/20-A", "Violet B 705/70-A"), main = "CD4/CD8")
```

##### wadeTools method
```{r, eval = TRUE, echo = TRUE, fig.show='hold'}
ff = get_sample(fn = filename)

# make a couple of pictures
pplot(ff, c("FSC-A", "SSC-A"), tx = 'linear', ty = 'linear', main = "Scattering")
pplot(ff, c("CD4PETR", "CD8Q705"), 
      xlim = bx(c(-1000, 2^18)), ylim = bx(c(-1000, 2^18)), 
      main = "CD4/CD8")
```

Comparing the two methods, the **wadeTools** approach accomplishes all of the work in a single line of code, using **get_sample()**. The **flowCore** approach on the other hand requires several lines, and requires that the user master (and remember) the somewhat arcane syntax of the transform methodology.  **flowCore** also has fairly primitive default graphics.

In **wadeTools**, the function **get_sample()** does a bunch of heavy lifting. It reads the file, applies the compensation matrix in the header, derails, does a linear transform on scattering parameters and a custom biexponential transformation on the fluorescence parameters, and swaps the conjugate names for the detector names.  Of course, inside this function there's lots going on, but the nice thing is that's kind of hidden from you and you needn't worry about it on a day-to-day basis.  And if you *do* want to worry about it you can always consult the source code!

Compare the two sets of figures and you'll notice that the labeling of fluorescence parameters in the **wadeTools** version is a lot more user-friendly than the default labeling with **flowCore**.  You'll also notice that the rendering of the distributions is a lot more like what you're accustomed to looking at (especially if you're a FlowJo user).  You will also note one more thing:  the default biexponential transform in flowCore is not appropriate.  It over-squashes the negatives, leading to a splitting around zero, which generates "false populations" (this really just means that the default parameters of the flowCore biexp are poorly chosen).  **This is a warning to be careful that your transformation makes sense.**

So, not only is the **wadeTools** approach much simpler from a code perspective, the result is quite a bit nicer (IMHO).

## Next: a simple pre-gating task 
It is often (always?) the case that raw data needs to be pre-gated prior to *any* sophisticated computational analysis.  For example, we're *never* interested in dead cells, or doublet events.  We often include dump markers to exclude cells that aren't the ones we're targeting. It's important to get rid of the junk so that it doesn't bias or confuse downstream algorithms (being careful not to throw out the baby with the bath water!). Thus, even though we'd like to think that gating is a thing of the past, it's not.

Here, we'll talk about *pre-gating*, which is a cleanup step prior to clustering or some other means of computationally analyzing the data (get rid of the bath water, keep the baby).  We'll use that term to distinguish what we're doing here from a conventional gating analysis, where the analyst specifies analytical gates that define all populations of interest, counting events inside those gates as the primary means of characterizing a sample and ignoring any events not in any of the analytical gates.  Of course, you will recognize this as "hypothesis-driven" data analysis, where the gates define all hypotheses to be tested.  Pre-gating is a preliminary step towards clustering analysis, which represents an "hypothesis-generating" approach.

What we aim to do is to create *algorithmic gates* that recognize and follow populations as they wander through multivariate space due to instrument or staining variability.  Our goal is to (a) find major populations in an unbiased fashion, (b) do it automatically, so it can be applied to 100's or 1000's of files without user intervention and (c) do it reproducibly.  Having accomplished this, we will persist the result of this *pre-gating* for further downstream processing via your clustering algorithm of choice (that's a computer geek way of saying, "we will save the results as, say, gated FCS files").

All that said, let's see if we can find the lymphocytes in our example file:

```{r, eval = TRUE, echo = TRUE, fig.width=5, fig.height=5}
# first get rid of debris, which can confuse blob.boundary
thresh_debris_fsc = 0.25
thresh_debris_ssc = 0.25
tmp = Subset(ff, rectangleGate("FSC-A" = c(thresh_debris_fsc, Inf), "SSC-A" = c(thresh_debris_ssc, Inf)))
bb = blob.boundary(ff = tmp, parameters = c("FSC-A", "SSC-A"), location = c(2, 1), height = .5, convex = TRUE)

pplot(ff, c("FSC-A","SSC-A"), tx = 'linear', ty = 'linear')
lines(bb)    # draw the blob contour on top of the figure
xline(thresh_debris_fsc, lty = 'dotdash')   # show the debris thresholds
yline(thresh_debris_ssc, lty = 'dotdash')

# check how many events are inside the gate
ff_gated = Subset(ff, polygonGate(.gate = bb))
nrow(ff)
nrow(ff_gated)

# Out of the 198k events in the original file, 102k are inside the blob boundary
```

**blob.boundary()** is limited by the contrast in the data.  In this case, we're looking for the blob nearest the location c(2, 1).  You'll see with your eyes that there's a blob that looks to be about at c(1.5, 1.0).  However, there's a subtle valley separating it from another, slightly dense patch to the left.  Since we've specified **convex = TRUE**, and **height = .5**, the function will find the blob we're looking for, and search for the largest convex contour that encloses it.  The nearby blob causes that search to end at a fairly tight contour.

You should play around with **blob.boundary()** on your data.  Specifically, try different height and location values.  Try setting convex to TRUE or FALSE, or log.transform to TRUE or FALSE (see the help page for **blob.boundary()**), and watch what happens.  Also experiment with pre-filtering (as in the above example) to eliminate confounding but uninteresting portions of the distribution.  There is certainly an art to effectively using **blob.boundary()**!

If you wanted this gate to be a bit more inclusive you could 'inflate' it a bit:
```{r, eval = TRUE, echo = TRUE, fig.width=5, fig.height=5}
bb_inflated = inflate.contour(bb, dist = 0.25)
pplot(ff, c("FSC-A","SSC-A"), tx = 'linear', ty = 'linear')
lines(bb)                                   # the original contour, in black
lines(bb_inflated, lwd = 3, col = 'red')    # the inflated contour, in red and heavy

# re-gate
ff_gated = Subset(ff, polygonGate(.gate = bb_inflated))

# re-count gated events
nrow(ff_gated)

# note that the number of gated events rose from 102k to 121k due to the fact
# that the gate is bigger, and thus encircles ~12% more events
```

Inflating is often a useful adjunct to **blob.boundary**, since it's pretty hard to find the outer limits of blobs.

## A More Complete Pre-gating Workflow
Let's now go back and design a complete pre-gating workflow for our example data file^[Please  note that this example file is a downsampled version (due to size constraints) of a data file in a FlowRepository project.  The original full dataset can be obtained from [FlowRepository](https://flowrepository.org/id/FR-FCM-ZZGS).  These data are too large to be included in an R package, but I encourage you to download the full dataset and explore these ideas on the real data.  I also **strongly recommend** using [FlowRepositoryR](http://bioconductor.org/packages/release/bioc/html/FlowRepositoryR.html) to download the data.].

Here is the flowFrame we imported using **get_sample()**:
```{r, eval = TRUE, echo = TRUE}
pData(parameters(ff))[, 1:2]
```
This looks like a T cell panel, but in order not to miss the NK and NKT cells we'll not pre-gate on CD3, but rather will save it as an analytical parameter.  Thus, a reasonable gating strategy might be as follows:

1. Eliminate non-stationary acquisition (**clean.fp()**)
1. Eliminate non-singlet events (using scattering -H and -W)
1. Keep the lymphocytes (using FSC-A and SSC-A)
1. Eliminate dead cells (using the LIVEDEAD marker)

##### Gate 1: Stationary acquisition

```{r, eval = TRUE, echo = TRUE, fig.width=7, fig.height=7}
# I will select SSC-A plus one parameter from each laser.
params = c("SSC-A", "KI67FITC", "CD127BV421", "CCR4AF647", "PD1PE")

# tag events in "bad" slices
tmp = clean.fp(ff = ff, parameters = params, show = TRUE)

# filter out the events in "bad" slices
ff_clean = Subset(tmp, rectangleGate("clean" = c(0.5, Inf)))

# summarize how much we lost
no = nrow(ff)
nc = nrow(ff_clean)
nd = no - nc
sprintf("eliminating %d events in bad slices (%.1f%%)", nd, 100 * nd / no)

```

##### Gate 2: Singlets
There are lots of ideas on how to do this.  Here's one:
```{r, eval = TRUE, echo = TRUE, fig.show='hold'}
###################
# FSC singlets
###################
pplot(ff_clean, c("FSC-H", "FSC-W"), tx = 'linear', ty = 'linear')
bb = blob.boundary(ff, parameters = c("FSC-H", "FSC-W"), height = .1, strongest = TRUE)
bb = get.hull(bb)                      # convex hull of bb
bb_infl = inflate.contour(bb, dist = .1)    # inflate a bit
fscw_thresh = max(bb_infl[, 2])             # col 2 is FSC-W
lines(bb)
lines(bb_infl, lwd = 2)
yline(fscw_thresh, lwd = 2, lty = 'dotdash', col = 'red')
ff_sing1 = Subset(ff_clean, rectangleGate("FSC-W" = c(-Inf, fscw_thresh)))


###################
# SSC singlets
###################
pplot(ff_sing1, c("SSC-H", "SSC-W"), tx = 'linear', ty = 'linear')
bb = blob.boundary(ff, parameters = c("SSC-H", "SSC-W"), height = .1, strongest = TRUE)
bb = get.hull(bb)                      # convex hull of bb
bb_infl = inflate.contour(bb, dist = .1)    # inflate a bit
fscw_thresh = max(bb_infl[, 2])             # col 2 is SSC-W
lines(bb)
lines(bb_infl, lwd = 2)
yline(fscw_thresh, lwd = 2, lty = 'dotdash', col = 'red')
ff_sing2 = Subset(ff_sing1, rectangleGate("SSC-W" = c(-Inf, fscw_thresh)))

# summarize how much we lost
no = nrow(ff_clean)
nc = nrow(ff_sing2)
nd = no - nc
sprintf("eliminating %d non-singlet events (%.1f%%)", nd, 100 * nd / no)
```

#####Gate 3: Lymphocytes
```{r, eval = TRUE, echo = TRUE, fig.width=7, fig.height=7}

# we'll do this just like in the earlier example
thresh_debris_fsc = 0.25
thresh_debris_ssc = 0.25
tmp = Subset(ff_sing2, rectangleGate("FSC-A" = c(thresh_debris_fsc, Inf), "SSC-A" = c(thresh_debris_ssc, Inf)))
bb = blob.boundary(ff = tmp, parameters = c("FSC-A", "SSC-A"), location = c(2, 1), height = .5, convex = TRUE)
bb_inflated = inflate.contour(bb, dist = 0.25)

pplot(ff, c("FSC-A","SSC-A"), tx = 'linear', ty = 'linear')
lines(bb)    # draw the blob contour on top of the figure
lines(bb_inflated, lwd = 3, col = 'red')    # the inflated contour, in red and heavy

xline(thresh_debris_fsc, lty = 'dotdash')   # show the debris thresholds
yline(thresh_debris_ssc, lty = 'dotdash')

ff_lymph = Subset(ff_sing2, polygonGate(.gate = bb_inflated))

```

#####Gate 4: Live events
```{r, eval = TRUE, echo = TRUE, fig.width=7, fig.height=7}
# We will illustrate the use of Kernel Density Estimation of 1D probability distributions
# to find a LIVEDEAD threshold
kde = bkde(exprs(ff_lymph)[, "LIVEDEAD"], bandwidth = 0.05)   # bkde is from package KernSmooth
kde = normalize.kde(kde)

# find valley(s)
res = find.local.minima(kde)

# if more than one minimum, find the lowest one above, say, 2000 on the LIVEDEAD axis
thresh_ld = res$x[min(which(res$x > bx(2000)))]

ff_gated = Subset(ff_lymph, rectangleGate("LIVEDEAD" = c(-Inf, thresh_ld)))

# visualize
pplot(ff_lymph, c("LIVEDEAD", "SSC-A"), ty = 'linear')
lines(kde$x, 5 * kde$y)     # factor of 5 just for visual appeal
xline(res$x, lty = 'dotdash')                # show all minima we found
xline(thresh_ld, lwd = 3, col = 'red')  # show the one we picked

# summarize how much we lost
no = nrow(ff_lymph)
nc = nrow(ff_gated)
nd = no - nc
sprintf("eliminating %d dead events (%.1f%%)", nd, 100 * nd / no)
```


## more to come

work on collections (flowSets)

add phenoData from spreadsheets

draw custom graphs using bx()




